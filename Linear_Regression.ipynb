{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is Simple Linear Regression ?"
      ],
      "metadata": {
        "id": "VhP0XAFebcsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and a single independent variable (X) using the equation:\n",
        "Y=mX+c\n",
        "Y=mX+c\n",
        "\n",
        "- where:\n",
        "\n",
        "    - mm = slope of the line\n",
        "    - cc = intercept (value of YY when X=0X=0)"
      ],
      "metadata": {
        "id": "bpgoYTQKbnDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.  What are the key assumptions of Simple Linear Regression ?"
      ],
      "metadata": {
        "id": "0anPX0mNbniy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Key Assumptions of Simple Linear Regression:\n",
        "\n",
        "    - Linearity – The relationship between X and Y is linear.\n",
        "    - Independence – Observations are independent of each other.\n",
        "    - Homoscedasticity – Constant variance of residuals.\n",
        "    - Normality of Residuals – Residuals should follow a normal distribution."
      ],
      "metadata": {
        "id": "suKadFPNbn4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. What does the coefficient m represent in the equation Y=mX+c ?"
      ],
      "metadata": {
        "id": "go6vqGtDcSsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the equation **\\( Y = mX + c \\)**, the coefficient **\\( m \\)** represents the **slope** of the line.  \n",
        "\n",
        "### **Interpretation of \\( m \\) (Slope):**  \n",
        "- It indicates the **rate of change** of \\( Y \\) with respect to \\( X \\).  \n",
        "- It tells us how much \\( Y \\) increases or decreases when \\( X \\) increases by **one unit**.  \n",
        "\n",
        "### **Example:**  \n",
        "If the equation is:  \n",
        "\\[\n",
        "Y = 3X + 5\n",
        "\\]  \n",
        "- The **slope \\( m = 3 \\)** means that for every **1-unit increase in \\( X \\), \\( Y \\) increases by 3**.  \n",
        "- If \\( m \\) were negative (e.g., \\( Y = -2X + 7 \\)), it would mean that **as \\( X \\) increases, \\( Y \\) decreases by 2** per unit increase in \\( X \\).  \n",
        "\n",
        "### **General Cases:**  \n",
        "- **\\( m > 0 \\)** → Positive relationship (upward slope).  \n",
        "- **\\( m < 0 \\)** → Negative relationship (downward slope).  \n",
        "- **\\( m = 0 \\)** → No relationship (horizontal line).  \n",
        "\n"
      ],
      "metadata": {
        "id": "EIb8wHaqcZbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.  What does the intercept c represent in the equation Y=mX+c ?"
      ],
      "metadata": {
        "id": "85gsAqmhc82D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Interpretation of the Intercept (\\( c \\)) in the Equation \\( Y = mX + c \\)**  \n",
        "\n",
        "The intercept **\\( c \\)** represents the **value of \\( Y \\) when \\( X = 0 \\)**. It is the point where the regression line crosses the **Y-axis**.  \n",
        "\n",
        "### **Key Points About the Intercept (\\( c \\))**  \n",
        "- It is the predicted value of **\\( Y \\)** when **\\( X = 0 \\)**.  \n",
        "- It helps in understanding the starting point of the relationship between \\( X \\) and \\( Y \\).  \n",
        "- In real-world applications, if \\( X = 0 \\) has no meaning, the intercept may not always be useful.  \n",
        "\n",
        "### **Example 1:**  \n",
        "If the regression equation is:  \n",
        "\\[\n",
        "Y = 3X + 5\n",
        "\\]  \n",
        "- When \\( X = 0 \\), \\( Y = 5 \\).  \n",
        "- This means the intercept **\\( c = 5 \\)** is the value of \\( Y \\) when there is no contribution from \\( X \\).  \n",
        "\n",
        "### **Example 2 (Real-world Scenario):**  \n",
        "Consider a salary prediction model:  \n",
        "\\[\n",
        "\\text{Salary} = 5000 \\times \\text{Years of Experience} + 30000\n",
        "\\]  \n",
        "- Here, **\\( c = 30,000 \\)** means that even **without any experience (Years of Experience = 0), the predicted base salary is 30,000**.  \n",
        "\n",
        "### **Special Cases:**  \n",
        "- If **\\( c = 0 \\)**, it means that when \\( X = 0 \\), \\( Y \\) is also 0.  \n",
        "- If \\( c \\) is **negative**, the model predicts a negative outcome for \\( Y \\) when \\( X = 0 \\), which may not always be meaningful.  \n",
        "\n"
      ],
      "metadata": {
        "id": "ytvmUfhOdbvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. How do we calculate the slope m in Simple Linear Regression ?\n",
        "\n",
        "### **How to Calculate the Slope \\( m \\) in Simple Linear Regression**  \n",
        "\n",
        "The slope \\( m \\) in the equation **\\( Y = mX + c \\)** represents the rate of change in \\( Y \\) for every unit increase in \\( X \\). It determines how strongly \\( X \\) influences \\( Y \\).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Formula for \\( m \\) (Slope):**  \n",
        "\\[\n",
        "m = \\frac{N\\sum(XY) - \\sum X \\sum Y}{N\\sum X^2 - (\\sum X)^2}\n",
        "\\]  \n",
        "\n",
        "where:  \n",
        "- \\( N \\) = Total number of data points  \n",
        "- \\( \\sum XY \\) = Sum of the product of \\( X \\) and \\( Y \\)  \n",
        "- \\( \\sum X \\) = Sum of all \\( X \\) values  \n",
        "- \\( \\sum Y \\) = Sum of all \\( Y \\) values  \n",
        "- \\( \\sum X^2 \\) = Sum of squares of \\( X \\) values  \n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Explanation:**  \n",
        "1. **Find the required summations**:  \n",
        "   - Compute \\( \\sum X \\), \\( \\sum Y \\), \\( \\sum XY \\), and \\( \\sum X^2 \\) using the dataset.  \n",
        "2. **Apply the formula** to compute \\( m \\).  \n",
        "3. **Interpret the result**:  \n",
        "   - If \\( m > 0 \\), \\( Y \\) increases as \\( X \\) increases (positive relationship).  \n",
        "   - If \\( m < 0 \\), \\( Y \\) decreases as \\( X \\) increases (negative relationship).  \n",
        "   - If \\( m = 0 \\), there is no relationship between \\( X \\) and \\( Y \\).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example Interpretation:**  \n",
        "If the computed slope \\( m = 2.2 \\), it means that for **each unit increase in \\( X \\), \\( Y \\) increases by 2.2**.  \n",
        "\n"
      ],
      "metadata": {
        "id": "fM74jskjdihV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.  What is the purpose of the least squares method in Simple Linear Regression"
      ],
      "metadata": {
        "id": "gLDHSOUwdiZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Least Squares Method is used in Simple Linear Regression to find the best-fitting line by minimizing the sum of the squared errors (differences between actual and predicted values).\n",
        "- Why is it Needed?\n",
        "\n",
        "    - When fitting a straight line to data, multiple lines could pass through the points.\n",
        "    - The Least Squares Method ensures that the chosen line has the smallest possible total error.\n",
        "    - It helps find the optimal values of mm (slope) and cc (intercept) in the equation:\n",
        "    - Y=mX+c\n",
        "    - Y=mX+c"
      ],
      "metadata": {
        "id": "LNfq_MAWdiRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression"
      ],
      "metadata": {
        "id": "sdplInsWdiJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Interpretation of the Coefficient of Determination (R2R2) in Simple Linear Regression\n",
        "\n",
        "- The coefficient of determination (R2R2) is a measure of how well the regression line fits the data. It indicates the proportion of variation in the dependent variable (YY) that is explained by the independent variable (XX).\n",
        "- Formula for R2\n",
        "  - R2=1−∑(Yactual−Ypredicted)2/∑(Yactual−Y)2\n",
        "\n",
        "\n",
        "- where:\n",
        "\n",
        "    - Yactual​ = Actual values of Y\n",
        "    - Ypredicted​ = Predicted values from the regression model\n",
        "    - Y = Mean of YY\n",
        "    - Numerator: Sum of squared residuals (unexplained variation)\n",
        "    - Denominator: Total sum of squares (total variation in Y)"
      ],
      "metadata": {
        "id": "JlxIm8X0diBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. What is Multiple Linear Regression ?"
      ],
      "metadata": {
        "id": "bwlg_WtFdh2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Multiple Linear Regression (MLR)?**  \n",
        "\n",
        "**Multiple Linear Regression (MLR)** is an extension of **Simple Linear Regression**, where there are **two or more independent variables** (\\( X_1, X_2, X_3, \\dots \\)) used to predict the dependent variable (\\( Y \\)).  \n",
        "\n",
        "---\n",
        "\n",
        "### **General Equation for MLR:**  \n",
        "\n",
        "\\[\n",
        "Y = b_0 + b_1X_1 + b_2X_2 + b_3X_3 + \\dots + b_nX_n + \\epsilon\n",
        "\\]\n",
        "\n",
        "where:  \n",
        "- \\( Y \\) = Dependent variable (target)  \n",
        "- \\( b_0 \\) = Intercept (value of \\( Y \\) when all \\( X \\) variables are 0)  \n",
        "- \\( b_1, b_2, b_3, \\dots, b_n \\) = Regression coefficients (show how each \\( X \\) affects \\( Y \\))  \n",
        "- \\( X_1, X_2, X_3, \\dots, X_n \\) = Independent variables (predictors)  \n",
        "- \\( \\epsilon \\) = Error term (captures variation not explained by the model)  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example Scenario:**  \n",
        "Imagine you are predicting **house prices (\\( Y \\))** based on:  \n",
        "- **Size of the house (\\( X_1 \\))** (in square feet)  \n",
        "- **Number of bedrooms (\\( X_2 \\))**  \n",
        "- **Location rating (\\( X_3 \\))** (scale from 1 to 10)  \n",
        "\n",
        "The equation might look like:  \n",
        "\n",
        "\\[\n",
        "\\text{Price} = 50 + 200X_1 + 100X_2 + 500X_3\n",
        "\\]\n",
        "\n",
        "**Interpretation:**  \n",
        "- **\\( X_1 \\) (Size):** For every extra square foot, price increases by \\$200.  \n",
        "- **\\( X_2 \\) (Bedrooms):** Each additional bedroom increases price by \\$100.  \n",
        "- **\\( X_3 \\) (Location Rating):** A higher location rating increases price by \\$500 per unit.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_nLEsO99dhRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. What is the main difference between Simple and Multiple Linear Regression"
      ],
      "metadata": {
        "id": "mM3V1D0qguEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "| Feature                     | Simple Linear Regression          | Multiple Linear Regression        |\n",
        "|-----------------------------|---------------------------------|---------------------------------|\n",
        "| **Number of Independent Variables** | One (\\(X\\)) | Two or more (\\(X_1, X_2, X_3, \\dots\\)) |\n",
        "| **Equation Form**           | \\( Y = mX + c \\) | \\( Y = b_0 + b_1X_1 + b_2X_2 + \\dots + b_nX_n + \\epsilon \\) |\n",
        "| **Example Scenario**        | Predicting house price based only on size | Predicting house price based on size, number of bedrooms, and location |\n",
        "| **Interpretation**          | Shows how one factor affects the dependent variable | Shows how multiple factors together influence the dependent variable |\n",
        "| **Complexity**              | Easier to interpret and visualize | More complex due to multiple predictors |\n",
        "| **Assumptions**             | Linearity, independence, homoscedasticity, normality of residuals | Linearity, independence, **multicollinearity**, homoscedasticity, normality of residuals |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bf3i_bPcg5hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.  What are the key assumptions of Multiple Linear Regression ?"
      ],
      "metadata": {
        "id": "sb6Q34dDgt_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "| **Assumption**            | **Description** | **How to Check** | **Possible Fixes** |\n",
        "|---------------------------|-----------------|------------------|-------------------|\n",
        "| **Linearity**             | Relationship between X and Y is linear | Scatter plots, residual plots | Transform variables (log, polynomial) |\n",
        "| **Independence of Errors** | Errors should not be correlated | Durbin-Watson test | Add missing variables, adjust model |\n",
        "| **No Multicollinearity**   | Predictors should not be highly correlated | VIF, correlation matrix | Remove correlated variables, use PCA, Ridge/Lasso |\n",
        "| **Homoscedasticity**       | Constant variance of residuals | Residual plots | Transform variables, Weighted Least Squares |\n",
        "| **Normality of Residuals** | Residuals should be normally distributed | Histogram, Q-Q plot, Shapiro-Wilk test | Apply log or Box-Cox transformation |\n",
        "| **No Omitted Variable Bias** | Important predictors should be included | Domain knowledge, statistical tests | Feature selection, add missing variables |"
      ],
      "metadata": {
        "id": "IpItJjNhgt4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model ?"
      ],
      "metadata": {
        "id": "pqecniOigtxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Heteroscedasticity refers to a condition in which the variance of the residuals (errors) in a regression model is not constant across all levels of the independent variables.\n",
        "\n",
        "-     In an ideal Multiple Linear Regression model, the residuals should be homoscedastic, meaning they have constant variance across all predicted values.\n",
        "    If heteroscedasticity is present, the spread of residuals increases or decreases systematically as the value of an independent variable changes.\n",
        "\n",
        "- How Does Heteroscedasticity Affect Regression Results?\n",
        "\n",
        "    - Biases in Standard Errors\n",
        "        Heteroscedasticity does not bias the estimated coefficients (ββ), but it affects their standard errors.\n",
        "        This leads to incorrect confidence intervals and p-values, making statistical tests unreliable.\n",
        "\n",
        "    - Inaccurate Hypothesis Testing\n",
        "        Because standard errors are incorrect, t-tests and F-tests become unreliable.\n",
        "        This can lead to false conclusions, such as wrongly rejecting or failing to reject a hypothesis.\n",
        "\n",
        "    - Reduced Prediction Accuracy\n",
        "        The model may perform well on some ranges of data but poorly on others, reducing its predictive reliability.\n",
        "\n",
        "    - Violates Assumptions of Ordinary Least Squares (OLS)\n",
        "        OLS regression assumes homoscedasticity for accurate coefficient estimation.\n",
        "        If heteroscedasticity exists, OLS estimates lose efficiency (they are no longer the best linear unbiased estimates, BLUE)."
      ],
      "metadata": {
        "id": "5KKQl9vjgton"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **12.  How can you improve a Multiple Linear Regression model with high multicollinearity**"
      ],
      "metadata": {
        "id": "03_mu6cTgtgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### **1. Remove Highly Correlated Predictors**  \n",
        "- Use **correlation matrix** or **Variance Inflation Factor (VIF)** to identify multicollinear variables.  \n",
        "- Drop one of the highly correlated predictors.\n",
        "\n",
        "### **2. Use Principal Component Analysis (PCA)**  \n",
        "- PCA transforms correlated features into **uncorrelated principal components**.  \n",
        "- Use these components instead of original variables.\n",
        "\n",
        "### **3. Apply Ridge or Lasso Regression**  \n",
        "- **Ridge Regression** (L2 regularization) reduces the impact of multicollinearity by shrinking coefficients.  \n",
        "- **Lasso Regression** (L1 regularization) can eliminate irrelevant features.\n",
        "\n",
        "### **4. Collect More Data**  \n",
        "- Increasing sample size can reduce multicollinearity effects.\n",
        "\n",
        "### **5. Combine Collinear Variables**  \n",
        "- Create a **new feature** by merging correlated variables (e.g., average, ratio, or interaction term).\n",
        "\n",
        "### **6. Center or Standardize Variables**  \n",
        "- Standardizing features (\\( X' = \\frac{X - \\mu}{\\sigma} \\)) helps in reducing multicollinearity effects.\n",
        "\n",
        "### **7. Use Domain Knowledge**  \n",
        "- Identify and keep only the most relevant predictors based on real-world significance.\n",
        "\n"
      ],
      "metadata": {
        "id": "wFcmtJTyik_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "- One-hot encoding, label encoding, ordinal encoding, and target encoding.\n"
      ],
      "metadata": {
        "id": "de3srleBgtWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "- They capture the combined effect of two or more variables on the dependent variable."
      ],
      "metadata": {
        "id": "ABHmT_MOjrn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "- In Simple Linear Regression, the intercept represents the dependent variable's value when the independent variable is zero.  \n",
        "- In Multiple Linear Regression, it represents the dependent variable's value when all independent variables are zero."
      ],
      "metadata": {
        "id": "8WFDURRzj19C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "- The slope indicates the rate of change in the dependent variable for a one-unit increase in the independent variable."
      ],
      "metadata": {
        "id": "qGYSyxKSj6o5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 17. What are the limitations of using R² as a sole measure of model performance?\n",
        "- It doesn’t account for overfitting, doesn’t work well with nonlinear relationships, and doesn’t indicate causation."
      ],
      "metadata": {
        "id": "eCG00uN0j_6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 18. How would you interpret a large standard error for a regression coefficient?\n",
        "- It suggests that the coefficient estimate is unstable and the variable may not be a strong predictor."
      ],
      "metadata": {
        "id": "DoHmwHZlkFQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 19. What is polynomial regression?\n",
        "- A type of regression where the relationship between the independent and dependent variable is modeled as an nth-degree polynomial."
      ],
      "metadata": {
        "id": "qbC1QbPjkHyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 20. When is polynomial regression used?\n",
        "- When the relationship between variables is nonlinear and a straight line doesn’t fit well."
      ],
      "metadata": {
        "id": "3uOo_m9GkMC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 21. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "- It gives the expected value of the dependent variable when all independent variables are zero."
      ],
      "metadata": {
        "id": "W-x3vcp_kR82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 22. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "- It appears as a funnel shape in residual plots; addressing it ensures reliable coefficient estimates and valid hypothesis testing."
      ],
      "metadata": {
        "id": "dFURVXlAkWrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 23. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "- It indicates that some independent variables may not contribute significantly to the model."
      ],
      "metadata": {
        "id": "D40mHNC8kYQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 24. Why is it important to scale variables in Multiple Linear Regression?\n",
        "- To ensure variables with different units or magnitudes don’t disproportionately affect the model."
      ],
      "metadata": {
        "id": "GIe-ogwykciF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 25. How does polynomial regression differ from linear regression?\n",
        "- Polynomial regression captures nonlinear relationships using polynomial terms, while linear regression models straight-line relationships"
      ],
      "metadata": {
        "id": "E4b35EukkfkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 26. What is the general equation for polynomial regression?\n",
        "- \\( y = b_0 + b_1x + b_2x^2 + ... + b_nx^n + \\epsilon \\)"
      ],
      "metadata": {
        "id": "tnm2T4xlkiRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 27. Can polynomial regression be applied to multiple variables?\n",
        "- Yes, by including polynomial terms for multiple independent variables."
      ],
      "metadata": {
        "id": "WxfAgzXGkocQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 28. What are the limitations of polynomial regression?\n",
        "- It can overfit, become complex with high-degree polynomials, and may not generalize well."
      ],
      "metadata": {
        "id": "BrJC27BUkrFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "- R², adjusted R², cross-validation, and residual analysis."
      ],
      "metadata": {
        "id": "4OciyVEBktZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 30. Why is visualization important in polynomial regression?\n",
        "- It helps to identify the best-fitting polynomial degree and detect overfitting.\n"
      ],
      "metadata": {
        "id": "V4uRtPG9kwHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 31. How is polynomial regression implemented in Python?\n",
        "- Using `PolynomialFeatures` from `sklearn.preprocessing`, then fitting a linear model to transformed data."
      ],
      "metadata": {
        "id": "T8tTPfR9kyIk"
      }
    }
  ]
}